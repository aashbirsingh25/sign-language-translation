## Sign Language Translation using Deep Learning & Multimodal Learning

A deep-learningâ€“powered system that translates sign language gestures into text (and optionally speech) using multimodal learning, combining video, keypoints, vision transformers, and sequence models. This project aims to make communication more accessible for people with hearing or speech impairments by providing real-time sign recognition and translation.

ðŸš€ Features

- Real-time sign language recognition using webcam or pre-recorded videos
- Multimodal learning using:

- Raw RGB frames

- Pose/keypoint extraction (MediaPipe / OpenPose / MMPose)

- Optical flow (optional)

- Deep learning models for robust translation:

- CNN + LSTM

- Transformers

- 3D-CNN or Video Vision Transformers (ViViT, TimeSformer)

- Text generation / sequence decoding for sentence-level translation

- Custom dataset support

- Training + inference scripts provided

- Optional text-to-speech output
